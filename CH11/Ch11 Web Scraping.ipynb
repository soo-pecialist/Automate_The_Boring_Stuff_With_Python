{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "webbrowser.open('http://inventwithpython.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project with webbrowser module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load map_it.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Wed Aug 21 20:44:01 2019\n",
    "\n",
    "@author: Soo Hyeon Kim\n",
    "Launches a map in the browser using an address from command line or clipboard.\n",
    "\"\"\"\n",
    "\n",
    "import webbrowser, sys, pyperclip\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "    # Get address from command line\n",
    "    address = ' '.join(sys.argv[1:])\n",
    "else:\n",
    "    # Get address from clipboard\n",
    "    address = pyperclip.paste()\n",
    "\n",
    "webbrowser.open('https://www.google.com/maps/place/'+address)\n",
    "\n",
    "## try belowh\n",
    "## 1)\n",
    "## !python3 map_it.py 870 Valencia St, San Francisco, CA 94110\n",
    "## 2) \n",
    "## copy: 870 Valencia St, San Francisco, CA 94110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Files from the Web with the requests Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.get('https://automatetheboringstuff.com/files/rj.txt')\n",
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.status_code == requests.codes.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "re-use it under the terms of the Projec\n"
     ]
    }
   ],
   "source": [
    "print(res.text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a problem: 404 Client Error: Not Found for url: http://inventwithpython.com/page_that_does_not_exist\n"
     ]
    }
   ],
   "source": [
    "res = requests.get('http://inventwithpython.com/page_that_does_not_exist')\n",
    "try:\n",
    "    res.raise_for_status()\n",
    "except Exception as exc:\n",
    "    print('There was a problem: {}'.format(exc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving downlaoded Files to the Hard Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://automatetheboringstuff.com/files/rj.txt')\n",
    "\n",
    "try:\n",
    "    res.raise_for_status()\n",
    "except Exception as exc:\n",
    "    print('There was a problem: {}'.format(exc))\n",
    "\n",
    "with open('Romeo_and_Juliet.txt', 'wb') as play_file:\n",
    "    for chunk in res.iter_content(100000):\n",
    "        play_file.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare\r",
      "\r\n",
      "\r",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r",
      "\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r",
      "\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r",
      "\r\n",
      "with this eBook or online at www.gutenberg.org/license\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "Title: Romeo and Juliet\r",
      "\r\n",
      "\r",
      "\r\n",
      "Author: William Shakespeare\r",
      "\r\n",
      "\r",
      "\r\n",
      "Posting Date: May 25, 2012 [EBook #1112]\r",
      "\r\n",
      "Release Date: November, 1997  [Etext #1112]\r",
      "\r\n",
      "\r",
      "\r\n",
      "Language: English\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK ROMEO AND JULIET ***\r",
      "\r\n",
      "\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 20 ./Romeo_and_Juliet.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML with the BeautifulSoup Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\r\n",
      "<html lang=\"en\" dir=\"ltr\">\r\n",
      "<head>\r\n",
      "<link rel=\"profile\" href=\"https://www.w3.org/1999/xhtml/vocab\" />\r\n",
      "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\r\n",
      "<link rel=\"shortcut icon\" href=\"https://nostarch.com/sites/default/files/favicon.ico\" type=\"image/vnd.microsoft.icon\" />\r\n",
      "<meta name=\"description\" content=\"“The best part of programming is the triumph of seeing the machine do something useful. Automate the Boring Stuff with Python frames all of programming as these small triumphs; it makes the boring fun.” —Hilary Mason, Founder of Fast Forward Labs and Data Scientist in Residence at Accel\" />\r\n",
      "<meta name=\"generator\" content=\"Drupal 7 (http://drupal.org)\" />\r\n",
      "<link rel=\"canonical\" href=\"https://nostarch.com/automatestuff\" />\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 ./example.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, bs4\n",
    "res = requests.get('http://nostarch.com')\n",
    "try:\n",
    "    res.raise_for_status()\n",
    "except Exception as exc:\n",
    "    print('There was a problem: {}'.format(exc))\n",
    "\n",
    "nostarch_soup = bs4.BeautifulSoup(res.text)\n",
    "type(nostarch_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_file = open('example.html')\n",
    "example_soup = bs4.BeautifulSoup(example_file)\n",
    "type(example_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding an Element with the select() Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_file = open('example.html')\n",
    "example_soup = bs4.BeautifulSoup(example_file.read())\n",
    "elems = example_soup.select('p')\n",
    "type(elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p class=\"cart-block-items collapsed uc-cart-empty\">There are no products in your shopping cart.</p>'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(elems[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are no products in your shopping cart.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elems[0].getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(open('example.html'))\n",
    "span_elem = soup.select('span')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"sr-only\">Toggle navigation</span>,\n",
       " <span class=\"icon-bar\"></span>,\n",
       " <span class=\"icon-bar\"></span>,\n",
       " <span class=\"icon-bar\"></span>,\n",
       " <span class=\"input-group-btn\"><button class=\"btn btn-primary\" type=\"submit\"><span aria-hidden=\"true\" class=\"icon glyphicon glyphicon-search\"></span>\n",
       " </button></span>,\n",
       " <span aria-hidden=\"true\" class=\"icon glyphicon glyphicon-search\"></span>,\n",
       " <span class=\"input-group-btn\"><button class=\"btn btn-primary\" type=\"submit\"><span aria-hidden=\"true\" class=\"icon glyphicon glyphicon-search\"></span>\n",
       " </button></span>,\n",
       " <span aria-hidden=\"true\" class=\"icon glyphicon glyphicon-search\"></span>,\n",
       " <span class=\"cta-button\">Get The Bundle</span>,\n",
       " <span class=\"glyphicon glyphicon-menu-down pull-left\"></span>,\n",
       " <span class=\"btn-text\">Topics</span>,\n",
       " <span class=\"glyphicon glyphicon-menu-down pull-right\"></span>,\n",
       " <span class=\"field-content\"><a href=\"/catalog/art-photography-design\">Art &amp; Design</a></span>,\n",
       " <span class=\"field-content\"><a href=\"/catalog/general-computing\">General Computing</a></span>,\n",
       " <span class=\"field-content\"><a href=\"/catalog/security\">Hacking &amp; Computer Security</a></span>,\n",
       " <span class=\"field-content\"><a href=\"/catalog/hardware-and-diy\">Hardware / DIY</a></span>,\n",
       " <span class=\"field-content\"><a href=\"/catalog/kids\">Kids</a></span>,\n",
       " <span class=\"field-content\"><a href=\"/catalog/lego\">LEGO®</a></span>,\n",
       " <span class=\"field-content\"><a href=\"/catalog/linux-bsd-unix\">Linux &amp; BSD</a></span>,\n",
       " <span class=\"field-content\"><a href=\"/catalog/manga\">Manga</a></span>,\n",
       " <span class=\"field-content\"><a href=\"/catalog/programming\">Programming</a></span>,\n",
       " <span class=\"field-content\"><a href=\"/catalog/python\">Python</a></span>,\n",
       " <span class=\"field-content\"><a href=\"/catalog/science-math\">Science &amp; Math</a></span>,\n",
       " <span class=\"field-content\"><a href=\"/catalog/scratch\">Scratch</a></span>,\n",
       " <span class=\"field-content\"><a href=\"/catalog/system-administration\">System Administration</a></span>,\n",
       " <span class=\"field-content\"><a href=\"/catalog/early-access\">Early Access</a></span>,\n",
       " <span class=\"cart-block-icon-empty\" title=\"View your shopping cart.\"></span>,\n",
       " <span class=\"cart-block-title-bar\" title=\"Show/hide shopping cart contents.\">Shopping cart<span class=\"cart-block-arrow arrow-down\"></span></span>,\n",
       " <span class=\"cart-block-arrow arrow-down\"></span>,\n",
       " <span class=\"num-items\">0</span>,\n",
       " <span class=\"uc-price\">$0.00</span>,\n",
       " <span aria-hidden=\"true\" class=\"icon glyphicon glyphicon-plus\"></span>,\n",
       " <span class=\"footer-divider\">|</span>,\n",
       " <span class=\"footer-divider\">|</span>,\n",
       " <span class=\"footer-divider\">|</span>,\n",
       " <span class=\"footer-divider\">|</span>,\n",
       " <span class=\"footer-divider\">|</span>,\n",
       " <span class=\"footer-divider\">|</span>,\n",
       " <span class=\"footer-divider\">|</span>,\n",
       " <span class=\"footer-divider\">|</span>,\n",
       " <span class=\"footer-divider\">|</span>,\n",
       " <span class=\"footer-divider\">|</span>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<span class=\"sr-only\">Toggle navigation</span>'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(span_elem[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(span_elem[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_elem[0].get('some_nonexistent_addr') == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': ['sr-only']}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_elem[0].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"sr-only\">Toggle navigation</span>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('.sr-only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open('example.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"sr-only\">Toggle navigation</span>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(bs4.element.NavigableString, str)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup.span.string), type(soup.span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Comment"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup = \"<b><!--Hey, buddy. Want to buy a used parser?--></b>\"\n",
    "soup = BeautifulSoup(markup)\n",
    "comment = soup.b.string\n",
    "type(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b>\n",
      " <!--Hey, buddy. Want to buy a used parser?-->\n",
      "</b>\n"
     ]
    }
   ],
   "source": [
    "print(soup.b.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "<html><head><title>The Dormouse's story</title></head>\n",
       "<body>\n",
       "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
       "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       "and they lived at the bottom of a well.</p>\n",
       "<p class=\"story\">...</p>\n",
       "</body></html>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.body.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b>The Dormouse's story</b>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.body.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<head><title>The Dormouse's story</title></head>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_tag = soup.head\n",
    "head_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_tag.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', <html><head><title>The Dormouse's story</title></head>\n",
       " <body>\n",
       " <p class=\"title\"><b>The Dormouse's story</b></p>\n",
       " <p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       " <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       " and they lived at the bottom of a well.</p>\n",
       " <p class=\"story\">...</p>\n",
       " </body></html>]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'html'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.contents[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<head><title>The Dormouse's story</title></head>, '\\n', <body>\n",
       " <p class=\"title\"><b>The Dormouse's story</b></p>\n",
       " <p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       " <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       " and they lived at the bottom of a well.</p>\n",
       " <p class=\"story\">...</p>\n",
       " </body>]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.contents[1].contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<head><title>The Dormouse's story</title></head>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_tag = soup.contents[1].contents[0]\n",
    "head_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<head><title>The Dormouse's story</title></head>, '\\n', <body>\n",
       " <p class=\"title\"><b>The Dormouse's story</b></p>\n",
       " <p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       " <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       " and they lived at the bottom of a well.</p>\n",
       " <p class=\"story\">...</p>\n",
       " </body>]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.contents[1].contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tag = head_tag.contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_tag.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'head'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_tag.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n'\n",
      "\"The Dormouse's story\"\n",
      "'\\n'\n",
      "'\\n'\n",
      "\"The Dormouse's story\"\n",
      "'\\n'\n",
      "'Once upon a time there were three little sisters; and their names were\\n'\n",
      "'Elsie'\n",
      "',\\n'\n",
      "'Lacie'\n",
      "' and\\n'\n",
      "'Tillie'\n",
      "';\\nand they lived at the bottom of a well.'\n",
      "'\\n'\n",
      "'...'\n",
      "'\\n'\n"
     ]
    }
   ],
   "source": [
    "for string in soup.strings:\n",
    "    print(repr(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The Dormouse's story\"\n",
      "\"The Dormouse's story\"\n",
      "'Once upon a time there were three little sisters; and their names were'\n",
      "'Elsie'\n",
      "','\n",
      "'Lacie'\n",
      "'and'\n",
      "'Tillie'\n",
      "';\\nand they lived at the bottom of a well.'\n",
      "'...'\n"
     ]
    }
   ],
   "source": [
    "for string in soup.stripped_strings:\n",
    "    print(repr(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head><title>The Dormouse's story</title></head>\n",
       "<body>\n",
       "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
       "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       "and they lived at the bottom of a well.</p>\n",
       "<p class=\"story\">...</p>\n",
       "</body></html>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tag.string.parent.parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "<html><head><title>The Dormouse's story</title></head>\n",
       "<body>\n",
       "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
       "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       "and they lived at the bottom of a well.</p>\n",
       "<p class=\"story\">...</p>\n",
       "</body></html>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and\\n'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.a.next_sibling.next_sibling.next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a', class_='sister', id='link3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
       "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       "and they lived at the bottom of a well.</p>\n",
       "<p class=\"story\">...</p>\n",
       "</body>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tag.string.next_element.next_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Dormouse's story\""
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tag.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a', 'sister', id=\"link2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a', {'class': 'sister'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"body strikeout\"></p>]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "css_soup = BeautifulSoup('<p class=\"body strikeout\"></p>')\n",
    "css_soup.find_all(\"p\", class_=\"strikeout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"body strikeout\"></p>]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, os\n",
    "css_soup.find_all(\"p\", class_=re.compile(r'body'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elsie']"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(string='Elsie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "<html><head><title>The Dormouse's story</title></head>\n",
       "<body>\n",
       "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
       "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       "and they lived at the bottom of a well.</p>\n",
       "<p class=\"story\">...</p>\n",
       "</body></html>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elsie', 'Lacie', 'Tillie']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(string=re.compile(r'ie'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elsie', 'Lacie', 'Tillie']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(string=['Elsie', 'Lacie', 'Tillie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<head><title>The Dormouse's story</title></head>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: \"I'm Feeling Lucky\" Google Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load lucky.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Thu Aug 22 11:23:36 2019\n",
    "\n",
    "@author: Soo Hyeon Kim\n",
    "\n",
    "- Get search keywords from the command line arguments\n",
    "- Retrieve the search results page\n",
    "- Open a browser tab for each result\n",
    "\"\"\"\n",
    "\n",
    "import requests, sys, webbrowser\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def open_for_me():\n",
    "    arg=input(\"Your search phrase? \")\n",
    "    \n",
    "    print('Googling...')    # display text while downloading the Google page\n",
    "    res = requests.get('https://www.google.com/search?q=' + ' '.join(arg))\n",
    "    \n",
    "    try:\n",
    "        res.raise_for_status()\n",
    "    except Exception as exc:\n",
    "        print('There was a problem: {}'.format(exc))\n",
    "        \n",
    "    # TODO: Retrieve top search result links\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    \n",
    "    # Open a browser tab for each result\n",
    "    link_elems = soup.select('.r a')\n",
    "    num_open = min(5, len(link_elems))\n",
    "    for i in range(num_open):\n",
    "        webbrowser.open('http://google.com' + link_elems[i].get('href'))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your search phrase? atlanta falcon\n",
      "Googling...\n"
     ]
    }
   ],
   "source": [
    "open_for_me()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Downloading All XKCD Comics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading page http://xkcd.com...\n",
      "Downloading image http://imgs.xkcd.com/comics/review.png...\n",
      "Downloading page http://xkcd.com/2191/...\n",
      "Downloading image http://imgs.xkcd.com/comics/conference_question.png...\n",
      ".\n",
      ".\n",
      ".\n",
      "Downloading page http://xkcd.com/2/...\n",
      "Downloading image http://imgs.xkcd.com/comics/tree_cropped_(1).jpg...\n",
      "Downloading page http://xkcd.com/1/...\n",
      "Downloading image http://imgs.xkcd.com/comics/barrel_cropped_(1).jpg...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# %load downloading_comics.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Thu Aug 22 14:01:58 2019\n",
    "\n",
    "@author: Soo Hyeon Kim\n",
    "- loads the XKCD home page.\n",
    "- Saves the comic image on the page\n",
    "- Follows the Prvious Comic link\n",
    "- Repeats until it reaches the first comic\n",
    "\"\"\"\n",
    "\n",
    "import requests, os, bs4\n",
    "\n",
    "url = 'http://xkcd.com'\n",
    "os.makedirs('xkcd', exist_ok=True)\n",
    "\n",
    "while not url.endswith('#'):\n",
    "    # TODO: Donload the page.\n",
    "    print('Downloading page {}...'.format(url))\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "    \n",
    "    # TODO: Find the URL of the comic image\n",
    "    comic_elem = soup.select('#comic img') #inside <div> w/ id=comic, img\n",
    "    if comic_elem == []:\n",
    "        print(\"Couldn't find comic image.\")\n",
    "    else:\n",
    "        comic_url = 'http:' + comic_elem[0].get('src')\n",
    "        # Download the image.\n",
    "        print('Downloading image {}...'.format(comic_url))\n",
    "        \n",
    "        try:\n",
    "            res = requests.get(comic_url)\n",
    "        except Exception as ecc:\n",
    "            print('There was a problem: {}'.format(ecc))\n",
    "            \n",
    "        try:\n",
    "            res.raise_for_status()\n",
    "        except Exception as ecc:\n",
    "            print('There was a problem: {}'.format(ecc))\n",
    "    \n",
    "        # TODO: Save the image to ./xkcd\n",
    "        image_file = open(os.path.join('xkcd', os.path.basename(comic_url)), 'wb')\n",
    "        for chunk in res.iter_content(100000):\n",
    "            image_file.write(chunk)\n",
    "        image_file.close()\n",
    "        \n",
    "            \n",
    "    # TODO: Get the Prev button's url\n",
    "    prev_link = soup.select('a[rel=\"prev\"]')[0]\n",
    "    url = 'http://xkcd.com' + prev_link.get('href')\n",
    "    \n",
    "    \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling the Browser with the selenium Module\n",
    "\n",
    "### Starting a Selenium_Controlled Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, bs4, re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "browser = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get('http://inventwithpython.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selenium.webdriver.chrome.webdriver.WebDriver"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found <img> element with that class name!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    elem = browser.find_element_by_class_name('card-img-top')\n",
    "    print('Found <{}> element with that class name!'.format(elem.tag_name))\n",
    "except:\n",
    "    print('Was not able to find an element with that name.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clicking the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selenium.webdriver.remote.webelement.WebElement"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_elem = browser.find_element_by_link_text('Read Online for Free')\n",
    "type(link_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_elem.click() # follows the \"Read Online for Free\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling out and submitting forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome()\n",
    "browser.get(\"https://gmail.com\")\n",
    "\n",
    "email_elem = browser.find_element_by_id('identifierId')\n",
    "email_elem.send_keys('oonya77@gmail.com')\n",
    "\n",
    "next_button_elem = browser.find_element_by_css_selector(\"div[role=button]\")\n",
    "next_button_elem.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "password_elem = browser.find_element_by_css_selector(\"input[type=password]\")\n",
    "password_elem.send_keys(\"rla1tn2guS345!!\")\n",
    "## 1)\n",
    "# password_elem.submit()\n",
    "##2)\n",
    "# next_button_elem = browser.find_element_by_css_selector(\"div[role=button][id=passwordNext]\")\n",
    "# next_button_elem.click()\n",
    "## 3)\n",
    "password_elem.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_elem = browser.find_element_by_css_selector('html')\n",
    "html_elem.send_keys(Keys.END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_elem.send_keys(Keys.HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Projects\n",
    "\n",
    "## Command Line Emailer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to emailer program! This program will help you send an email on terminal without opening your web browser. We will need your gmail address, receiver's email address, your password, subject, and message.\n",
      "\n",
      "What is your google email address? myaccount@gmail.com\n",
      "What is receiver's email address? youraccount@gmail.com\n",
      "Password (shows as *): ········\n",
      "\n",
      "Enter your email subject: From emailer program\n",
      "Please type your message below. When done type 'SEND' or 'send':\n",
      "\n",
      "Hi, \n",
      "this is Sean.\n",
      "I am sending this email through my program\n",
      "hope this finds you well. Bye.\n",
      "\n",
      "Sincerely,\n",
      "Sean\n",
      "Send\n"
     ]
    }
   ],
   "source": [
    "# %load Command\\ Line\\ Emailer.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Thu Aug 22 17:49:49 2019\n",
    "\n",
    "@author: Soo Hyeon Kim\n",
    "\n",
    "takes an email address and string of text on the command line and, \n",
    "logs into your email account and sends and email of the string to \n",
    "the provided address.\n",
    "\"\"\"\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import getpass, time\n",
    "\n",
    "def command_line_emailer():\n",
    "    welcome = \"\"\"\n",
    "    Welcome to emailer program! This program will help you send\n",
    "    an email on terminal without opening your web browser. We will\n",
    "    need your gmail address, receiver's email address, your password,\n",
    "    subject, and message.\"\"\"\n",
    "    print(re.sub(\" +\", \" \", welcome.replace('\\n', '')).strip())\n",
    "    print()\n",
    "    \n",
    "    sender = input(\"What is your google email address? \")\n",
    "    receiver = input(\"What is receiver's email address? \")\n",
    "    password = getpass.getpass('Password (shows as *): ')\n",
    "    print()\n",
    "    subject = input(\"Enter your email subject: \" )\n",
    "    print(\"Please type your message below. When done type 'SEND' or 'send':\\n\")\n",
    "    \n",
    "    lines = []\n",
    "    while True:\n",
    "        line = input()\n",
    "        if line.lower() != 'send':\n",
    "            lines.append(line)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    message = '\\n'.join(lines)\n",
    "    \n",
    "#    print(sender, receiver, password)\n",
    "#    print(message)\n",
    "    \n",
    "    # open a browser and log into your email\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get('https://gmail.com')\n",
    "    \n",
    "    email_elem = browser.find_element_by_css_selector('#identifierId')\n",
    "    email_elem.send_keys(sender)\n",
    "    \n",
    "    next_button_elem = browser.find_element_by_css_selector(\"div[role=button]\")\n",
    "    next_button_elem.click()\n",
    "            \n",
    "    \n",
    "    time.sleep(1.5) # page open delay\n",
    "    \n",
    "    password_elem = browser.find_element_by_css_selector(\"input[type=password]\")\n",
    "    password_elem.send_keys(password)\n",
    "\n",
    "    password_elem.send_keys(Keys.RETURN)\n",
    "    \n",
    "    time.sleep(1.5) # page open delay\n",
    "    \n",
    "    # let's compose email\n",
    "    compose_elem = browser.find_element_by_css_selector('.z0 div[role=button]')\n",
    "    compose_elem.click()\n",
    "    \n",
    "    # recipient \n",
    "    time.sleep(1.5) # page open delay\n",
    "    receiver_elem = browser.find_element_by_name('to')\n",
    "    receiver_elem.send_keys(receiver)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # subject\n",
    "    subject_elem = browser.find_element_by_name('subjectbox')\n",
    "    subject_elem.send_keys(subject)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # message\n",
    "    subject_elem.send_keys(Keys.TAB + message)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # sent!\n",
    "    subject_elem.send_keys(Keys.TAB +Keys.TAB + Keys.ENTER)\n",
    "\n",
    "    \n",
    "                                                      \n",
    "if __name__ == \"__main__\":\n",
    "    command_line_emailer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Site Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default search keyword is 'Atlanta', you want ot change? (y/n) what?\n",
      "I'm sorry. You must input 'y' or 'n'\n",
      "Default search keyword is 'Atlanta', you want ot change? (y/n) y\n",
      "What do you want to search? kids\n",
      "Start to donload images on https://imgur.com/search?q=kids\n",
      "Stored folder name: kids_images\n",
      "Downloading image https://i.imgur.com/6ZGPtOhb.jpg ...\n",
      "Downloading image https://i.imgur.com/8UQhIQQb.jpg ...\n",
      "Downloading image https://i.imgur.com/TWCZtEab.jpg ...\n",
      "Downloading image https://i.imgur.com/1vSpxbTb.jpg ...\n",
      "Downloading image https://i.imgur.com/4nMlzKKb.jpg ...\n",
      "Downloading image https://i.imgur.com/J5wwUp8b.jpg ...\n",
      "Downloading image https://i.imgur.com/TNgyOmzb.jpg ...\n",
      "Downloading image https://i.imgur.com/NEs33qSb.jpg ...\n",
      "Downloading image https://i.imgur.com/tf4022Ub.jpg ...\n",
      "Downloading image https://i.imgur.com/GaCKufmb.jpg ...\n",
      "Downloading image https://i.imgur.com/RMbGCmnb.jpg ...\n",
      "Downloading image https://i.imgur.com/Vj9KtoIb.jpg ...\n",
      "Downloading image https://i.imgur.com/2kiQZyAb.jpg ...\n",
      "Downloading image https://i.imgur.com/WDCamlLb.jpg ...\n",
      "Downloading image https://i.imgur.com/iNZEU4ib.jpg ...\n",
      "Downloading image https://i.imgur.com/0ayovGHb.jpg ...\n",
      "Downloading image https://i.imgur.com/I3d1rU9b.jpg ...\n",
      "Downloading image https://i.imgur.com/lHar1Dib.jpg ...\n",
      "Downloading image https://i.imgur.com/jfVg8rBb.jpg ...\n",
      "Downloading image https://i.imgur.com/eA7JeTJb.jpg ...\n",
      "Downloading image https://i.imgur.com/p4VFgobb.jpg ...\n",
      "Downloading image https://i.imgur.com/RL9w082b.jpg ...\n",
      "Downloading image https://i.imgur.com/HXxhc8Ab.jpg ...\n",
      "Downloading image https://i.imgur.com/nlV69mBb.jpg ...\n",
      "Downloading image https://i.imgur.com/rOXpHJtb.jpg ...\n",
      "Downloading image https://i.imgur.com/mM8sBjlb.jpg ...\n",
      "Downloading image https://i.imgur.com/66WxJDeb.jpg ...\n",
      "Downloading image https://i.imgur.com/pPa7aKWb.jpg ...\n",
      "Downloading image https://i.imgur.com/MLlz5SKb.jpg ...\n",
      "Downloading image https://i.imgur.com/34uTQxub.jpg ...\n",
      "Downloading image https://i.imgur.com/32r4LRBb.jpg ...\n",
      "Downloading image https://i.imgur.com/Mie9qjZb.jpg ...\n",
      "Downloading image https://i.imgur.com/kieXWEOb.jpg ...\n",
      "Downloading image https://i.imgur.com/7PMHI9Hb.jpg ...\n",
      "Downloading image https://i.imgur.com/oRLuoL5b.jpg ...\n",
      "Downloading image https://i.imgur.com/epWhPrJb.jpg ...\n",
      "Downloading image https://i.imgur.com/j8LMhgtb.jpg ...\n",
      "Downloading image https://i.imgur.com/Kw8lD13b.jpg ...\n",
      "Downloading image https://i.imgur.com/N5IWMlyb.jpg ...\n",
      "Downloading image https://i.imgur.com/7NeYSqkb.jpg ...\n",
      "Downloading image https://i.imgur.com/XIqsX6ab.jpg ...\n",
      "Downloading image https://i.imgur.com/at7lGoNb.jpg ...\n",
      "Downloading image https://i.imgur.com/Gh9UR70b.jpg ...\n",
      "Downloading image https://i.imgur.com/K9OljZub.jpg ...\n",
      "Downloading image https://i.imgur.com/HfUXKjRb.jpg ...\n",
      "Downloading image https://i.imgur.com/mMiXT3mb.jpg ...\n",
      "Downloading image https://i.imgur.com/HJAA0Vzb.jpg ...\n",
      "Downloading image https://i.imgur.com/tM7P4A8b.jpg ...\n",
      "Downloading image https://i.imgur.com/71sxNl2b.jpg ...\n",
      "Downloading image https://i.imgur.com/TSGArVEb.jpg ...\n",
      "Downloading image https://i.imgur.com/UaTn1A5b.jpg ...\n",
      "Downloading image https://i.imgur.com/N3pgVYqb.jpg ...\n",
      "Downloading image https://i.imgur.com/PDq3vatb.jpg ...\n",
      "Downloading image https://i.imgur.com/sGyeY8Ub.jpg ...\n",
      "Downloading image https://i.imgur.com/erRjKT9b.jpg ...\n",
      "Downloading image https://i.imgur.com/U6l3HqJb.jpg ...\n",
      "Downloading image https://i.imgur.com/Hz9i7EBb.jpg ...\n",
      "Downloading image https://i.imgur.com/UoJReTzb.jpg ...\n",
      "Downloading image https://i.imgur.com/UeFMdigb.jpg ...\n",
      "Downloading image https://i.imgur.com/WHdvNmhb.jpg ...\n"
     ]
    }
   ],
   "source": [
    "# %load ImageSiteDownloader.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Thu Aug 22 20:37:52 2019\n",
    "\n",
    "@author: Soo Hyeon Kim\n",
    "program goes to a photo-sharing site like flickr or Imgur, \n",
    "searches for a category of photos, and then downloads all \n",
    "the resulting images. \n",
    "\"\"\"\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import time, os\n",
    "import requests\n",
    "\n",
    "\n",
    "def imgur_image_downloader(keyword='Atlanta'):\n",
    "    # get a keyword for search\n",
    "    if keyword == 'Atlanta':\n",
    "        while True:\n",
    "            reply = input(\"Default search keyword is 'Atlanta', you want ot change? (y/n) \")\n",
    "            if reply == 'y':\n",
    "                keyword = input(\"What do you want to search? \")\n",
    "                break\n",
    "            elif reply not in ['y', 'n', 'yes', 'no']:\n",
    "                print(\"I'm sorry. You must input 'y' or 'n'\")\n",
    "            else:\n",
    "                print(\"Okay. Go with no change\")\n",
    "                break\n",
    "\n",
    "    keyword = keyword.lower()\n",
    "\n",
    "    # input into search box\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get('https://imgur.com')\n",
    "    \n",
    "    search_box = browser.find_element_by_css_selector('input[type=text][class=Searchbar-textInput]')\n",
    "    search_box.send_keys(keyword + Keys.ENTER)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    url = browser.current_url\n",
    "    \n",
    "    res  = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    \n",
    "    # retrieve all image elements\n",
    "    image_elem = soup.select('.image-list-link img')\n",
    "    img_urls = ['https:'+img.get('src') for img in image_elem]\n",
    "    \n",
    "    print(\"Start to donload images on {}\".format(url))\n",
    "    file_name = keyword+'_images'\n",
    "    print(\"Stored folder name: {}\".format(file_name))\n",
    "    \n",
    "    # make directory for images\n",
    "    os.makedirs(file_name, exist_ok=True)\n",
    "    \n",
    "    for img_url in img_urls:\n",
    "        print('Downloading image {} ...'.format(img_url))\n",
    "        res = requests.get(img_url)\n",
    "        res.raise_for_status()\n",
    "        \n",
    "        with open(os.path.join(file_name, os.path.basename(img_url)), 'wb') as image_file:\n",
    "            for chunk in res.iter_content(100000):\n",
    "                image_file.write(chunk)\n",
    "                \n",
    "#### Test *******             \n",
    "imgur_image_downloader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's play 2048. Default round number is 100.\n",
      "You want to change it? (y/n) y\n",
      "How many rounds? 200\n",
      "Game Over\n"
     ]
    }
   ],
   "source": [
    "# %load play_2048.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Thu Aug 22 21:53:29 2019\n",
    "\n",
    "@author: Soo Hyeon Kim\n",
    "\n",
    "# http://gabrielecirulli.github.io/2048\n",
    "Let program play 2048 own its on. \n",
    "\"\"\"\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import random, re, time\n",
    "\n",
    "def play_2048(rounds=100):\n",
    "    print(\"Let's play 2048. Default round number is {}.\".format(rounds))\n",
    "    reply = input(\"You want to change it? (y/n) \")\n",
    "    \n",
    "    # set round\n",
    "    while reply != \"n\":\n",
    "        rounds = input(\"How many rounds? \")\n",
    "        try:\n",
    "            rounds = int(rounds)\n",
    "        except:\n",
    "            print(\"Hey, type number\")\n",
    "            continue\n",
    "    \n",
    "        reply = 'n'\n",
    "            \n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(\"http://gabrielecirulli.github.io/2048\")\n",
    "    html_elem = browser.find_element_by_tag_name('html')\n",
    "    \n",
    "    while rounds > 0:\n",
    "        arrow = random.choice([Keys.UP, Keys.RIGHT, Keys.DOWN, Keys.LEFT])\n",
    "        html_elem.send_keys(arrow)\n",
    "        rounds -= 1\n",
    "        time.sleep(0.25)\n",
    "    \n",
    "    print(\"Game Over\")\n",
    "        \n",
    "\n",
    "#### Test *****\n",
    "play_2048()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating ... this may take a while. Relax...\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.cbc.ca/ideas/episodes/2014/01/15/the-great-book-of-knowledge-part-1/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.cbsnews.com/news/wikipedia-jimmy-wales-morley-safer-60-minutes/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.dw.de/inside-wikipedia-attack-of-the-pr-industry/av-17745881\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/List_of_women_writer\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Statistical_measurement\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://wikimediadc.org/wiki/Home\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.signonsandiego.com/uniontrib/20041206/news_mz1b6encyclo.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://rogchap.com/2011/09/06/top-40-website-programming-languages/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.time.com/time/business/article/0,8599,1595184,00.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://weblogs.hitwise.com/bill-tancer/2007/03/wikipedia_search_and_school_ho.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.comscore.com/Insights/Press_Releases/2012/9/comScore_Media_Metrix_Ranks_Top_50_US_Web_Properties_for_August_201\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.ecommercetimes.com/story/76351.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.cityweekly.net/utah/article-5129-feature-wikipediots-who-are-these-devoted-even-obsessive-contributors-to-wikipedia.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://legacy.utsandiego.com/news/tech/personaltech/20061009-9999-mz1b9wikiped.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.cbsnews.com/news/wikipedia-jimmy-wales-morley-safer-60-minutes/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://adsabs.harvard.edu/abs/2005Natur.438..900G\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://news.bbc.co.uk/2/hi/technology/4530930.stm\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://content.time.com/time/specials/packages/article/0,28804,1975813_1975844_1976488,00.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://historynewsnetwork.org/article/125437\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://educationnext.org/wikipedia-or-wickedpedia/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.niemanlab.org/2011/10/the-contribution-conundrum-why-did-wikipedia-succeed-while-other-encyclopedias-failed/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://cits.tamiu.edu/kock/pubs/journals/2016JournalIJeC_WikipediaEcollaboration/Kock_etal_2016_IJeC_WikipediaEcollaboration.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.signonsandiego.com/uniontrib/20041206/news_mz1b6encyclo.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://features.slashdot.org/features/05/04/18/164213.shtml\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.nupedia.com/pipermail/nupedia-l/2001-January/000684.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.nupedia.com/pipermail/nupedia-l/2001-January/000676.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.networksolutions.com/whois/results.jsp?domain=wikipedia.com\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.networksolutions.com/whois/results.jsp?domain=wikipedia.org\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.wikipedia.com/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://osdir.com/ml/science.linguistics.wikipedia.international/2003-03/msg00008.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.wikisym.org/ws2009/procfiles/p108-suh.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.bostonreview.net/books-ideas/edit-page-wikipedia-evgeny-morozov\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://snap.stanford.edu/class/cs341-2012/reports/09-GibbonsVetranoBiancaniCS341.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://libresoft.es/publications/thesis-jfelipe\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://www.independent.co.uk/life-style/gadgets-and-tech/news/wikipedia-seeks-women-to-balance-its-geeky-editors-2333605.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.technologyreview.com/featuredstory/520446/the-decline-of-wikipedia/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.pcworld.com/article/129135/wikipedia_breaks_into_us_top_10_sites.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.alexa.com/siteinfo/wikipedia.org\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://latimesblogs.latimes.com/technology/2012/01/wikipedia-sopa-blackout-congressional-representatives.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://wikimediafoundation.org/wiki/SOPA/Blackoutpage\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://economictimes.indiatimes.com/articleshow/29094246.cms\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.alexa.com/topsites\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.businessinsider.com/pending-changes-safeguard-on-wikipedia-2012-12\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://w3.linux-magazine.com/issue/51/Wikipedia_Encyclopedia.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://firstmonday.org/article/view/1108/1028\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://repository.upenn.edu/cgi/viewcontent.cgi?article=1508&context=cis_papers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://alumni.media.mit.edu/~fviegas/papers/history_flow.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www-users.cs.umn.edu/~reid/papers/group282-priedhorsky.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://archive.firstamendmentcenter.org/news.aspx?id=17798\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.businessweek.com/stories/2005-12-13/wikipedia-a-work-in-progress\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://sys03-public.nbcnews.com/technology/wikipedia-editorial-warzone-says-study-838793\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://ieeexplore.ieee.org/document/7408171/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://portal.acm.org/citation.cfm?doid=1641309.1641322\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://link.springer.com/10.1007/s11199-015-0573-y\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.pcworld.idg.com.au/index.php/id;1866322157;fp;2;fpid;2\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://features.slashdot.org/story/05/04/18/164213/the-early-history-of-nupedia-and-wikipedia-a-memoir\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://firstmonday.org/ojs/index.php/fm/article/view/2613/2479\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.boston.com/business/technology/articles/2006/02/13/many_contributors_common_cause\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.research.ibm.com/visual/papers/wikipedia_coordination_final.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.cnn.com/2003/TECH/internet/08/03/wikipedia/index.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.kuro5hin.org/story/2004/12/30/142458/25\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.public.iastate.edu/~goodwin/pubs/goodwinwikipedia.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.businessinsider.com/2009/1/who-the-hell-writes-wikipedia-anyway\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.slate.com/id/2184487\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.aaronsw.com/weblog/whowriteswikipedia\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.sciam.com/article.cfm?id=good-samaritans-are-on-the-money\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.cbc.ca/news/canada/toronto/ocad-to-storm-wikipedia-this-fall-1.1412807\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.research.ibm.com/visual/papers/viegas_hicss_visual_wikipedia.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://adsabs.harvard.edu/abs/2012PLoSO...730091Y\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.slate.com/articles/technology/future_tense/2014/06/wikipedia_s_bureaucracy_problem_and_how_to_fix_it.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://chronicle.com/article/The-Undue-Weight-of-Truth-on/130704/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://faculty.washington.edu/jwj/lis521/colon_wikipedia.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.bgsu.edu/news/2012/02/wikipedia-experience-sparks-national-debate.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://oliverkamm.typepad.com/blog/2007/08/wisdom-more-lik.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.wikipedia-watch.org/psamples.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://archive.wired.com/culture/lifestyle/news/2005/12/69844\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://corporate.britannica.com/britannica_nature_response.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.nature.com/press_releases/Britannica_response.pdf?item\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.nature.com/nature/britannica/index.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.plosone.org/annotation/listThread.action?root%3D80078\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.pcworld.com/article/170874/the_15_biggest_wikipedia_blunders.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.tnr.com/story.html?id=82eb5d70-13bd-4086-9ec0-cb0e9e8411b3\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://many.corante.com/archives/2005/01/04/academia_and_wikipedia.php\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.dw.de/inside-wikipedia-attack-of-the-pr-industry/av-17745881\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.citizendium.org/essay.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://news.cnet.com/8301-10784_3-6032713-7.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.msnbc.msn.com/id/16775981\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.cc.com/video-clips/z1aahs/the-colbert-report-the-word---wikiality\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.emorywheel.com/detail.php?n=17902\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://adsabs.harvard.edu/abs/1985CACM...28...22S\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.insidehighered.com/news/2007/01/26/wiki\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.businessweek.com/technology/content/dec2005/tc20051214_441708.htm\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.thecrimson.com/article.aspx?ref=517305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.timeshighereducation.co.uk/story.asp?sectioncode=26&storycode=209408\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://chnm.gmu.edu/essays-on-history-new-media/essays/?essayid=42\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.economist.com/node/8820422?story_id=8820422\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.economist.com/printedition/displaystory.cfm?story_id=10789354\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://in.reuters.com/article/technologyNews/idINIndia-32865420080405\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www-users.cs.umn.edu/~echi/papers/2009-CHI2009/p1509.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://files.grouplens.org/papers/wp-gender-wikisym2011.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://works.bepress.com/cgi/viewcontent.cgi?article=1035&context=laura_quilter\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.smh.com.au/technology/technology-news/wikipedia-rejects-child-porn-accusation-20100428-tsvh\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.theinquirer.net/inquirer/news/1603521/wikipedia-denies-child-abuse-allegations\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://economictimes.indiatimes.com/infotech/internet/Wikipedia-blasts-co-founders-accusations-of-child-porn-on-website/articleshow/5871943.cms\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.smh.com.au/technology/technology-news/wikipedia-rejects-child-porn-accusation-20100428-tsvh.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://news.bbc.co.uk/2/hi/technology/10104946.stm\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://newswire.xbiz.com/view.php?id=169017\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.heise.de/newsticker/meldung/Gericht-weist-einstweilige-Verfuegung-gegen-Wikimedia-Deutschland-ab-Update-173587.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.otrs.com/en/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.slate.com/articles/technology/bitwise/2014/12/wikipedia_editing_disputes_the_crowdsourced_encyclopedia_has_become_a_rancorous.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.csmonitor.com/World/Security-Watch/Backchannels/2013/0801/In-UK-rising-chorus-of-outrage-over-online-misogyny\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.nedworks.org/~mark/presentations/san/Wikimedia_architecture.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://news.softpedia.com/news/Wikipedia-s-New-VisualEditor-Is-the-Best-Update-in-Years-and-You-Can-Make-It-Better-365072.shtml\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://stream.aljazeera.com/story/201407211855-0023944\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://www.toolserver.org/~leon/stats/reqstats/reqstats-monthly.png\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.globule.org/publi/WWADH_comnet2009.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.computerworld.com/s/article/9116787/Wikipedia_simplifies_IT_infrastructure_by_moving_to_one_Linux_vendor?taxonomyId=154&pageNumber=1&taxonomyName=Servers%20and%20Data%20Center\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.datacenterknowledge.com/archives/2013/01/14/its-official-equinix-ashburn-is-wikimedias-home/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://blogs.bing.com/site_blogs/b/search/archive/2009/07/27/researching-with-bing-reference.aspxResearching\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.wikipediaondvd.com/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.wikipediaondvd.com/site.php?temp=buy\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://python.org.ar/pyar/Proyectos/CDPedia\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://schools-wikipedia.org\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.faz.net/s/RubCF3AEB154CE64960822FA5429A182360/Doc~E7A20980B9C0D46E99A9F60BC09506343~ATpl~Ecommon~Scontent.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.slis.indiana.edu/news/story.php?story_id=2064\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://androgeoid.com/2011/04/local-points-of-interest-in-wikipedia\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.ilounge.com/index.php/articles/comments/15802\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.niemanlab.org/2013/01/wikipedia-plans-to-expand-mobile-access-around-the-globe-with-new-funding\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.comscore.com/press/release.asp?press=849\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://weblogs.hitwise.com/leeann-prescott/2007/02/wikipedia_traffic_sources.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://weblogs.hitwise.com/leeann-prescott/2006/10/wikipedia_and_academic_researc.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.pewinternet.org/pdfs/PIP_Wikipedia07.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.businessinsider.com/2011-digital-100#7-wikimedia-foundation-wikipedia-7\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.ca11.uscourts.gov/opinions/ops/200216886.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.parl.gc.ca/LegisInfo/BillDetails.aspx?billId=1585203&View=10\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.ibls.com/internet_law_news_portal_view.aspx?s=latestnews&id=1668\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.ajr.org/Article.asp?id=4461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.economist.com/node/21530100\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.japannewsreview.com/society/chubu/20070705page_id=364\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.mysanantonio.com/news/metro/stories/MYSA010307.02A.richter.132c153.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://archives.starbulletin.com/2006/01/13/news/story03.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://content.time.com/time/magazine/article/0,9171,1570810,00.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.bbc.co.uk/programmes/b007tc6x\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.grillini.it/show.php?4885\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.webbyawards.com/webbys/winners-2004.php\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.brandchannel.com/features_effect.asp?pf_id=352\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://loomarea.com/die_quadriga/e/index.php?title=Award_2008\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.erasmusprijs.org/?lang=en&page=Erasmusprijs\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.fpa.es/es/premios-princesa-de-asturias/premiados/2015-wikipedia.html?especifica=0&idCategoria=0&anio=2015&especifica=0\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.lne.es/sociedad-cultura/2015/10/22/fundadores-wikipedia-destacan-version-asturiano/1830529.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.theonion.com/articles/wikipedia-celebrates-750-years-of-american-indepen,2007/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.theonion.com/articles/la-law-wikipedia-page-viewed-874-times-today,18521/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.officetally.com/the-office-the-negotiation\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://usatoday30.usatoday.com/tech/webguide/internetlife/2007-04-12-office-wikipedia_N.htm\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.collegehumor.com/video/3581424/professor-wikipedia\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://dilbert.com/strips/comic/2009-05-08\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.comedy.org.uk/guide/radio/bigipedia/interview/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://tosh.comedycentral.com/blog/2010/02/03/your-wikipedia-entries\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://tosh.comedycentral.com/video-clips/wikipedia-updates\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.condenaststore.com/-sp/Dammit-Manning-have-you-considered-the-pronoun-war-that-this-is-going-t-Cartoon-Prints_i9813981_.htm\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.thetimes.co.uk/tto/opinion/letters/article4639755.ece\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.thetimes.co.uk/tto/public/sitesearch.do?querystring=john%20julius%20norwich&p=tto&pf=all&bl=on\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://mediadecoder.blogs.nytimes.com//2012/03/13/after-244-years-encyclopaedia-britannica-stops-the-presses/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.gizmocrazed.com/2012/03/encyclopedia-britannica-dies-at-the-hands-of-wikipedia-infographic/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.roughtype.com/archives/2005/10/the_amorality_o.php\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.nature.com/nature/peerreview/debate/nature04992.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.cse.unt.edu/~tarau/teaching/NLP/papers/Mihalcea-2007-Wikify-Linking_Documents_to_Encyclopedic.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://staff.science.uva.nl/~mdr/Publications/Files/linkkdd2005.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.technologyreview.com/view/544266/wikipedia-mining-algorithm-reveals-worlds-most-influential-universities/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.marketwatch.com/story/two-universities-beat-harvard-in-this-surprising-school-ranking-2015-12-09\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.domesday1986.com/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.cnn.com/2000/TECH/computing/11/21/net.gen.encyclopedias.idg/index.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.crmbuyer.com/story/53137.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.americanhistoryprojects.com/downloads/JMH1812.PDF\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://adsabs.harvard.edu/abs/2012PLoSO...7E0091Y\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://blog.ericgoldman.org/archives/2010/02/catching_up_wit.htm\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.firstmonday.org/issues/issue12_8/nielsen/index.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://jcmc.indiana.edu./vol12/issue1/pfeil.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://portal.acm.org/citation.cfm?doid=1316624.1316663\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://reagle.org/joseph/2007/10/Wikipedia-Authorial-Leadership.pdf\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.firstmonday.org/issues/issue12_4/wilkinson/index.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://adsabs.harvard.edu/abs/2007cs........2140W\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://reagle.org/joseph/2010/gfc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.cbc.ca/ideas/episodes/2014/01/15/the-great-book-of-knowledge-part-1/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.cbc.ca/ideas/popupaudio.html?clipIds=2430203709\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://blogs.chron.com/brokenrecord/2008/03/for_music_fans_wikipedia_myspa.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://thephoenix.com/Boston/Life/52864-Wikipedia-rules\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.lrb.co.uk/v31/n10/runc01_.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.time.com/time/magazine/article/0,9171,1066904-1,00.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.economist.com/science/tq/displaystory.cfm?story_id=11484062\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://www.independent.co.uk/life-style/gadgets-and-tech/features/is-wikipedia-cracking-up-1543527.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.technologyreview.com/featuredstory/520446/the-decline-of-wikipedia/\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.capitalnewyork.com/article/city-hall/2015/03/8563947/edits-wikipedia-pages-bell-garner-diallo-traced-1-police-plaza\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://fullmeasure.news/news/politics/dark-side-of-wikipedia\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://topics.nytimes.com/top/news/business/companies/wikipedia/index.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://www.ted.com/index.php/talks/jimmy_wales_on_the_birth_of_wikipedia.html\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://isni.org/isni/000000044914788X\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://aleph.nli.org.il/F/?func=find-b&local_base=NNL10&find_code=SYS&con_lng=eng&request=001395032\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Wikipediahttp://katalog.nsk.hr/F/?func=direct&doc_number=000625468&local_base=nsk10\n",
      "There was a problem with the url: 404 Client Error: Not Found for url: https://www.worldcat.org/identities/containsVIAFID/195846295\n",
      "Done!\n",
      "3428 links were scanned from 'https://en.wikipedia.org/wiki/Wikipedia'\n",
      "200 errors of type '404' found\n"
     ]
    }
   ],
   "source": [
    "# %load link_verification.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Thu Aug 22 23:54:21 2019\n",
    "\n",
    "@author: Soo Hyeon Kim\n",
    "Given the URL, the program will attempt to download every linked page on the \n",
    "page.  The program should flag any pages that have a 404 \"Not Found\" status \n",
    "code and print them out as broken links\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "main_url = \"https://en.wikipedia.org/wiki/Wikipedia\"\n",
    "errorCode = '404'\n",
    "\n",
    "res = requests.get(main_url)\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "link_elems = soup.select('a[href]')\n",
    "\n",
    "errors = []\n",
    "print(\"Validating ... this may take a while. Relax...\")\n",
    "\n",
    "\"\"\"\n",
    "if you want to check unique start of each links, try this:\n",
    "check = []\n",
    "for link in link_elems:\n",
    "    check.append(link.get('href')[:5])\n",
    "\n",
    "print(set(check)) # this should show the pattern\n",
    "\"\"\"\n",
    "for link in link_elems:\n",
    "    url = link.get('href')\n",
    "    \n",
    "    if url.startswith(\"https://\"):\n",
    "        try:\n",
    "            res = requests.get(url, timeout=0.5)\n",
    "            res.raise_for_status()\n",
    "        except Exception as exc:\n",
    "            if errorCode in str(exc.args[0]):\n",
    "                print(\"There was a problem with the url: {}\".format(exc))\n",
    "                errors.append(url)\n",
    "    \n",
    "    elif url.startswith(\"//\"):\n",
    "        try:\n",
    "            res = requests.get('http:'+url, timeout=0.5)\n",
    "            res.raise_for_status()\n",
    "        except Exception as exc:\n",
    "            if errorCode in str(exc.args[0]):\n",
    "                print(\"There was a problem with the url: {}\".format(exc))\n",
    "                errors.append(url)\n",
    "    \n",
    "    elif url.startswith(\"/w\"):\n",
    "        try:\n",
    "            res = requests.get(\"https://en.wikipedia.org\" + url, timeout=0.5)\n",
    "            res.raise_for_status()\n",
    "        except Exception as exc:\n",
    "            if errorCode in str(exc.args[0]):\n",
    "                print(\"There was a problem with the url: {}\".format(exc))\n",
    "                errors.append(url)\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            res = requests.get(\"https://en.wikipedia.org/wiki/Wikipedia\" \\\n",
    "                               + url, timeout=0.5)\n",
    "            res.raise_for_status()\n",
    "        except Exception as exc:\n",
    "            if errorCode in str(exc.args[0]):\n",
    "                print(\"There was a problem with the url: {}\".format(exc))\n",
    "                errors.append(url) \n",
    "            \n",
    "print(\"Done!\")\n",
    "print(\"{} links were scanned from '{}'\".format(len(link_elems), main_url))\n",
    "print(\"{} errors of type '404' found\".format(len(errors)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
